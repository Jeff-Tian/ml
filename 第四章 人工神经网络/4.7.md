# 4.7

考虑一个两层的前瞻 ANN，它具有两个输入 a 和 b，一个隐藏单元 c，和一个输出单元 d。这个网络有五个权值（w<sub>ca</sub>，w<sub>cb</sub>，w<sub>c0</sub>，w<sub>dc</sub>，w<sub>d0</sub>），其中 w<sub>x0</sub> 表示单元 x 的阈值权。先把这些权的值初始化为（0.1，0.1，0.1，0.1，0.1），然后给出使用反向传播算法训练这个网络的前两次迭代中每一次这些权值的值。假定学习速率 $|\eta = 0.3|$，冲量$|\alpha = 0.9|$，采用增量的权值更新和以下训练样例：

<pre>
    a    b    d
    1    0    1
    0    1    0
</pre>

---

| 迭代 | 权值 |
| --- | --- |
| 0   | (0.1, 0.1, 0.1, 0.1, 0.1) |
| 1   | (0.10085128181864877, 0.1, 0.1, 0.1189103977991797, 0.1) |
| 2   | (0.10161743545543266, 0.09899357900214031, 0.1, 0.1137618037587342, 0.1)

其中，第三个权值 0.1 和第五个权值 0.1 是阈值权，不在迭代更新范围内。其属于超参数，由人为指定，在迭代过程中保持不变。在某些其他语境中，其被称为偏置值。

在线运行这道题：https://runkit.com/jeff-tian/runkit-npm-jeff-tian-perceptron-for-nn 

<script src="https://embed.runkit.com" data-element-id="my-element"></script>
<div id="my-element">
const nn = require("@jeff-tian/perceptron/lib/NeuralNetwork")

const flat = (prev, next)=>[...prev, ...next]
const getFlattedWeights = weights => weights.reduce(flat).reduce(flat)
const getNthWeights = index => '(' + getFlattedWeights(network.weightsHistory[index].weights).join(', ') + ')'

const network = nn.backPropFor2LevelSigmoidUnitForwardNetwork([
    {
      x: [1, 0],
      t: 1,
    },
    {
      x: [0, 1],
      t: 0,
    },
  ], 0.3, 0.9)
console.log("初始权值 = " + getNthWeights(0))
console.log("第一次迭代后的权值 = " + getNthWeights(1))
console.log("第二次迭代后的权值 = " + getNthWeights(2))
</div>

在线查看这个 ANN：

<iframe src="https://playground-tmg7.pages.dev/#activation=sigmoid&batchSize=1&dataset=ml4Dot7&regDataset=reg-plane&learningRate=0.03&regularizationRate=10&noise=0&networkShape=1&seed=0.03556&showTestData=true&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false" style="border: 0; width: 100%; min-height: 800px;"></iframe>