# 4.6

简略解释为什么公式（4.10）中的 delta 法则仅是公式（4.7）表示的真正梯度下降法则的近似？

---

真正的梯度下降法则：
$$
\Delta w_i = \eta \sum_{d \in D}(t_d - o_d) x_{id} \tag{4.7}
$$

近似梯度下降法则：
$$
\Delta w_i = \eta (t-o)x_i \tag{4.10}
$$

仔细对比，可以发现（4.10）只考虑了当前的误差，而（4.7）则考虑了对于所有训练样例整体的误差。也就是说，采用（4.10），那么就是只使用当前一步的误差来指导机器学习，而（4.7）则看得更远。

![示意图](4.6/discount.png)