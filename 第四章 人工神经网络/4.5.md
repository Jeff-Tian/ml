# 4.5

推导输出为 o 的单个单元的梯度下降训练法则，其中

$$
o = w_0 + w_1 x_1 + w_1 x^2_1 + \dots + w_n x_n + w_n x^2_n
$$

---

我们怎样才能计算出沿着误差曲面最陡峭下降的方向呢？可以通过计算 $|E|$ 相对向量 $|\vec{w}|$ 的每个分量的导数来得到这个方向。

其中 $|E(\vec{w}) = \frac{1}{2} \sum_{d \in D} (t_d - o_d)^2|$。

这个向量导数被称为 $|E|$ 对于 $|\vec{w}|$ 的梯度，记作 $|\triangledown E(\vec{w})|$。

$$
\triangledown E(\vec{w}) \equiv [\frac{\partial	 E}{\partial	 w_0}, \frac{\partial	E}{\partial	 w_1}, \dots, \frac{\partial	E}{\partial	w_n}]
$$

当梯度被解释为权空间的一个向量时，它确定了使 E 最陡峭上升的方向。所以这个向量的反方向给出了最陡峭下降的方向。

既然梯度确定了 E 最陡峭上升的方向，那么梯度下降的训练法则是：

$$
\vec{w} \leftarrow	\vec{w} + \Delta \vec{w}
$$

其中：

$$
\Delta \vec{w} = -\eta \triangledown E(\vec{w})
$$

这里 $|\eta|$ 是一个正的常数叫做学习速率，它决定梯度下降搜索中的步长。公式中的负号是因为我们想让权向量向 E 下降的方向移动。这个训练法则也可以写成它的分量形式：

$$
w_i \leftarrow w_i + \Delta w_i
$$

其中：

$$
\Delta w_i = - \eta \frac{\partial E}{\partial w_i}                \tag{1}
$$

这样很清楚，最陡峭的下降可以按照比例 $|\frac{\partial E}{\partial w_i}|$ 改变 $|\vec{w}|$ 中的每一个分量$|w_i|$来实现。

要形成一个根据公式$|(1)|$迭代更新权值的实用算法，我们需要一个高效的方法在每一步都计算这个梯度，得到组成这个梯度向量的分量$|\frac{\partial E}{\partial w_i}|$的过程如下：

$$
\frac{\partial E}{\partial w_i} = \frac{\partial}{\partial w_i}\frac{1}{2}\sum_{d \in D}(t_d-o_d)^2 = \frac{1}{2}\sum_{d\in D}\frac{\partial}{\partial w_i}(t_d - o_d)^2 \\

= \frac{1}{2}\sum_{d \in D} 2 (t_d - o_d)\frac{\partial}{\partial w_i}(t_d - o_d) \\

= \sum_{d \in D}(t_d - o_d)\frac{\partial}{\partial w_i}(t_d - (w_{0} + w_{1} x_{1d} + w_1 x^2_{1d} + \dots + w_n x_{nd} + w_n x^2_{nd})) \\

= \sum_{d \in D}(t_d - o_d)(\frac{\partial}{\partial w_i} t_d - \frac{\partial}{\partial w_i} (w_0 + w_1 x_{1d} + w_1 x^2_{1d} + \dots + w_n x_{nd} + w_n x^2_{nd})) \\

= \sum_{d \in D}(t_d - o_d)(0 - \frac{\partial}{\partial w_i} (w_i x_{id} + w_i x^2_{id})) \\

\\

\frac{\partial E}{\partial w_i}=\sum_{d \in D}(t_d - o_d)(-x_{id}-x^2_{id}) = \sum_{d \in D}(t_d - o_d)(-x_{id})(1+x_{id})        \tag{2}
$$

其中，$|x_{id}|$ 表示训练样例$|d|$的一个输入分量$|x_i|$。现在我们有了一个公式，能够用线性单元的输入$|x_{id}|$、输出$|o_d|$以及训练样例的目标值 $|t_d|$ 表示 $|\frac{\partial E}{\partial w_i}|$。